{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      },
      "source": [
        "# Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "The objective of this ipynb is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.0)\n",
            "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.venv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.venv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.venv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.venv/lib/python3.10/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.venv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.venv/lib/python3.10/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.3)\n",
            "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qra06Ema_VL"
      },
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare\n",
        "\n",
        "1a). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) . Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) . Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "0348c430-6072-47c8-f599-f59a41c9e6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-04 18:30:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.16’\n",
            "\n",
            "input.txt.16        100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-04 18:30:55 (95.4 MB/s) - ‘input.txt.16’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "outputs": [],
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoMGZgEOdRjt",
        "outputId": "1b16ba7e-9303-4304-dccb-65fcd13e14c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1a) ['u', 'b', ':', 'A', 'I', 'p', 'h', ',', ' ', 'Y', \"'\", 'g', 'C', 's', '!', 'S', '?', 'R', 'O', 'z', 'B', 'k', 'j', 't', 'c', 'w', 'F', 'y', 'd', 'l', 'v', '\\n', 'M', ';', 'N', 'o', 'm', 'i', 'n', '.', 'W', 'r', 'f', 'a', 'e', 'L']\n",
            "an the the the the the the the the the the the the the the the the the the the the the the the the th\n"
          ]
        }
      ],
      "source": [
        "# 1a)\n",
        "chars = list(set(text))\n",
        "print(f\"1a) {chars}\")\n",
        "\n",
        "# 1b)\n",
        "c2id = {c: i for i, c in enumerate(chars)}\n",
        "id2c = {i: c for c, i in c2id.items()}\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [c2id[c] for c in s]\n",
        "\n",
        "# 1c)\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([id2c[i] for i in ids])\n",
        "\n",
        "# 1d)\n",
        "def create_one_hot_inputs_and_outputs(text) -> list[torch.tensor, torch.tensor]:\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    for i in range(len(text) - 1):\n",
        "        input_idx = encode(text[i])[0]\n",
        "        output_idx = encode(text[i + 1])[0]\n",
        "\n",
        "        input_one_hot = torch.zeros(len(chars))\n",
        "        output_one_hot = torch.zeros(len(chars))\n",
        "\n",
        "        input_one_hot[input_idx] = 1\n",
        "        output_one_hot[output_idx] = 1\n",
        "\n",
        "        inputs.append(input_one_hot)\n",
        "        outputs.append(output_one_hot)\n",
        "\n",
        "    inputs_one_hot = torch.stack(inputs)\n",
        "    outputs_one_hot = torch.stack(outputs)\n",
        "\n",
        "    return inputs_one_hot, outputs_one_hot\n",
        "\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs(text)\n",
        "# print(inputs_one_hot[1])\n",
        "# print(outputs_one_hot[0])\n",
        "\n",
        "# 1e)\n",
        "vocab_size = len(chars)\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(BigramOneHotMLP, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.fc1 = nn.Linear(vocab_size, 8)\n",
        "        self.fc2 = nn.Linear(8, vocab_size)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "          bigram_one_hot_mlp.eval()\n",
        "          with torch.no_grad():\n",
        "            current_char = start\n",
        "            word = current_char\n",
        "            for _ in range(max_new_tokens):\n",
        "                input_tensor = F.one_hot(torch.tensor(c2id[current_char]), vocab_size).unsqueeze(0).float()\n",
        "                output = bigram_one_hot_mlp(input_tensor)\n",
        "                next_char_id = torch.argmax(output).item()\n",
        "                next_char = [k for k, v in c2id.items() if v == next_char_id][0]\n",
        "                current_char = next_char\n",
        "                word += current_char\n",
        "            return word\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP(vocab_size=vocab_size)\n",
        "\n",
        "# 1f)\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "optimizer = optim.Adam(bigram_one_hot_mlp.parameters(), lr=0.1)\n",
        "\n",
        "# training loop\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    predict = bigram_one_hot_mlp(inputs_one_hot)\n",
        "    loss = F.cross_entropy(predict, outputs_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "print(bigram_one_hot_mlp.generate())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "PasrfDz-dSqx",
        "outputId": "61b2cf90-6f72-47a4-b1b4-14bd9231a292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "are the the the the the the the the the the the the the the the the the the the the the the the the t\n"
          ]
        }
      ],
      "source": [
        "# 1g)\n",
        "def create_embedding_inputs_and_outputs(text) -> list[torch.tensor, torch.tensor]:\n",
        "    input_ids = []\n",
        "    outputs = []\n",
        "    for i in range(len(text) - 1):\n",
        "        input_idx = encode(text[i])[0]\n",
        "        output_idx = encode(text[i + 1])[0]\n",
        "\n",
        "        output_one_hot = torch.zeros(len(chars))\n",
        "        output_one_hot[output_idx] = 1\n",
        "\n",
        "        input_ids.append(input_idx)\n",
        "        outputs.append(output_one_hot)\n",
        "\n",
        "    outputs_one_hot = torch.stack(outputs)\n",
        "\n",
        "\n",
        "    return torch.tensor(input_ids), outputs_one_hot\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs(text)\n",
        "# 1h)\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self, vocab_size,embedding_dim=8, hidden_size=8):\n",
        "        super(BigramEmbeddingMLP, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
        "        self.fc1 = nn.Linear(in_features=embedding_dim, out_features=hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        generated_text = start\n",
        "        for _ in range(max_new_tokens):\n",
        "            input_idx = torch.tensor([encode(generated_text[-1])[0]])\n",
        "            # print(encode(generated_text[-1])[0])\n",
        "            output = self.forward(input_idx)\n",
        "            next_token_id = torch.argmax(output).item()\n",
        "            next_char = decode([next_token_id])\n",
        "            generated_text += next_char\n",
        "        return generated_text\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP(vocab_size)\n",
        "\n",
        "optimizer = optim.Adam(bigram_embedding_mlp.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# training loop\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = bigram_embedding_mlp(input_ids)\n",
        "    # print(input_ids.shape, outputs.shape)\n",
        "    # break\n",
        "    loss = criterion(outputs, outputs_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(bigram_embedding_mlp.generate())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qplpM8_Cbp0s"
      },
      "source": [
        "## Part 2: Generative Pretrained Transformer\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Oh-3FeFxxnI",
        "outputId": "43efb1c6-60cd-4daf-a688-5b0ed27e48cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Feb  4 18:30:59 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L40S                    On  | 00000000:01:00.0 Off |                    0 |\n",
            "| N/A   51C    P0              88W / 350W |    587MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA L40S                    On  | 00000000:21:00.0 Off |                    0 |\n",
            "| N/A   37C    P8              34W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA L40S                    On  | 00000000:41:00.0 Off |                    0 |\n",
            "| N/A   36C    P8              34W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA L40S                    On  | 00000000:61:00.0 Off |                    0 |\n",
            "| N/A   32C    P8              34W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   4  NVIDIA L40S                    On  | 00000000:81:00.0 Off |                    0 |\n",
            "| N/A   32C    P8              32W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   5  NVIDIA L40S                    On  | 00000000:C1:00.0 Off |                    0 |\n",
            "| N/A   32C    P8              32W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   6  NVIDIA L40S                    On  | 00000000:E1:00.0 Off |                    0 |\n",
            "| N/A   29C    P8              31W / 350W |      5MiB / 46068MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A    305303      C   /home/zwzhu/dina/.venv/bin/python           578MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "outputs": [],
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      },
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string.\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids \n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnEOfMj4Dk4Y",
        "outputId": "ce4e144e-90a7-4227-f855-e4de5e966c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: ['q', '&', 'u', '-', 'b', ':', 'E', 'A', 'T', 'I', 'V', 'Q', 'p', 'h', ',', ' ', 'Y', \"'\", 'g', 'C', 's', '!', 'G', 'S', '?', '$', 'R', 'O', 'J', 'z', 'B', 'k', 'x', 'j', 't', 'c', 'w', 'F', 'y', 'X', 'U', 'P', 'd', 'l', 'v', 'K', '\\n', 'Z', 'M', '3', ';', 'N', 'D', 'o', 'H', 'm', 'i', 'n', '.', 'W', 'r', 'f', 'a', 'e', 'L']\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "chars = list(set(text))\n",
        "print(f\"1: {chars}\")\n",
        "\n",
        "# 2\n",
        "c2id = {c: i for i, c in enumerate(chars)}\n",
        "id2c = {i: c for c, i in c2id.items()}\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [c2id[c] for c in s]\n",
        "# 3\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([id2c[i] for i in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "7426b526-16c6-4b74-df48-1834d9fe6daa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63, 57,  5, 46, 30, 63],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 16\n",
        "data[:block_size+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvO4hSK171Vu"
      },
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "92eb9497-8307-4ee2-e8a4-d1e89fd1dec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([37], device='cuda:0') the target: 56\n",
            "when input is tensor([37, 56], device='cuda:0') the target: 60\n",
            "when input is tensor([37, 56, 60], device='cuda:0') the target: 20\n",
            "when input is tensor([37, 56, 60, 20], device='cuda:0') the target: 34\n",
            "when input is tensor([37, 56, 60, 20, 34], device='cuda:0') the target: 15\n",
            "when input is tensor([37, 56, 60, 20, 34, 15], device='cuda:0') the target: 19\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19], device='cuda:0') the target: 56\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56], device='cuda:0') the target: 34\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34], device='cuda:0') the target: 56\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56], device='cuda:0') the target: 29\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29], device='cuda:0') the target: 63\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63], device='cuda:0') the target: 57\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63, 57], device='cuda:0') the target: 5\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63, 57,  5],\n",
            "       device='cuda:0') the target: 46\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63, 57,  5, 46],\n",
            "       device='cuda:0') the target: 30\n",
            "when input is tensor([37, 56, 60, 20, 34, 15, 19, 56, 34, 56, 29, 63, 57,  5, 46, 30],\n",
            "       device='cuda:0') the target: 63\n"
          ]
        }
      ],
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "lFYZnm2MuLlt"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "# x,y = get_batch()\n",
        "# print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HmnXJjxtm3N"
      },
      "source": [
        "### Single Self Attention Head\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "5SD8Z16R-sfZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, feature_size = 64, head_size = 16):\n",
        "      super(SelfAttentionHead, self).__init__()\n",
        "      self.query = nn.Linear(feature_size, head_size)\n",
        "      self.key = nn.Linear(feature_size, head_size)\n",
        "      self.value = nn.Linear(feature_size, head_size)\n",
        "      self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      batch, timesteps, channels = x.shape\n",
        "      Q = self.query(x)\n",
        "      K = self.key(x)\n",
        "      V = self.value(x)\n",
        "\n",
        "      K_t = K.transpose(-2, -1)\n",
        "\n",
        "      attention_scores = torch.matmul(Q, K_t) / (K.size(-1) ** 0.5)\n",
        "\n",
        "      mask = torch.tril(torch.ones(timesteps, timesteps)).to('cuda')\n",
        "      masked_attention = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "      masked_attention = F.softmax(masked_attention, dim=-1)\n",
        "\n",
        "      attention_probs = self.dropout(masked_attention)\n",
        "      attention_output = torch.matmul(attention_probs, V)\n",
        "\n",
        "      return attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWeoHGBiFpWd"
      },
      "source": [
        "### Multihead Self Attention \n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "gFsPDkpnFs_b"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads=4, head_size=16, feature_size=64, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(feature_size, head_size) for _ in range(num_heads)])\n",
        "        self.linear = nn.Linear(num_heads * head_size, feature_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = [head(x) for head in self.heads]\n",
        "        concatenated_output = torch.cat(head_outputs, dim=-1)  \n",
        "        attention_output = self.linear(concatenated_output)\n",
        "        output = self.dropout(attention_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      },
      "source": [
        "## MLP \n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "K96Z3kAv7lNt"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=64, hidden_dim=256, output_dim=64, dropout_rate=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUFxuyf-JIxr"
      },
      "source": [
        "## Transformer block\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "xTDAd66KIvvx"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd=64, n_head=4, n_inner=256, dropout_rate=0.1):\n",
        "        super(Block, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embd // n_head, n_embd, dropout_rate)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = MLP(n_embd, n_inner, n_embd, dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFQXltDKNti"
      },
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` \n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. \n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` \n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      },
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "8WT4oUN084ts"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=64, n_head=4, n_layer=3, max_seq_len=128, dropout_rate=0.1):\n",
        "        super(GPT, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        # self.positional_embeddings = self.sin_position_encoding(seq_len=max_seq_len, d_model=n_embd).cuda()\n",
        "        self.positional_embeddings = nn.Embedding(max_seq_len, n_embd)\n",
        "        # print(self.positional_embeddings)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, n_embd * n_head, dropout_rate) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.position_indices = torch.arange(0, max_seq_len).unsqueeze(0).cuda()\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        token_embeddings = self.token_embedding(idx)\n",
        "        # print(f\"token_embeddings = {token_embeddings.shape}\")\n",
        "        # position_embeddings = self.position_embedding[:, :idx.size(1), :]\n",
        "        # positions = torch.arange(0, token_embeddings.size(1)).unsqueeze(0)\n",
        "        position_indices = self.position_indices[:, :token_embeddings.size(1)].expand(token_embeddings.size(0), -1)\n",
        "        # positions = positions.to(token_embeddings.device)\n",
        "        # print(position_indices.shape)\n",
        "        # print(position_indices)\n",
        "        position_embeddings = self.positional_embeddings(position_indices)\n",
        "        x = token_embeddings + position_embeddings #\n",
        "        # print(f\"x1: {x.shape}\")\n",
        "        x = self.blocks(self.dropout(x))\n",
        "        # print(f\"x2: {x.shape}\")\n",
        "        x = self.ln_f(x)\n",
        "        # print(f\"x3: {x.shape}\")\n",
        "        logits = self.head(x)\n",
        "        # print(f\"logits: {logits.shape}\")\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, start_char='th', max_new_tokens=32, top_p=0.9, top_k=5, temperature=1.0):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            generated_text = start_char\n",
        "            for _ in range(max_new_tokens):\n",
        "                # input_idx = torch.tensor([encode(generated_text[-1])[0]]).to('cuda')\n",
        "                input_idx = torch.tensor(encode(generated_text)[-block_size:]).to('cuda').reshape(1, -1)\n",
        "        \n",
        "                print(f\"Input: {decode(input_idx.reshape(-1).cpu().tolist())}\")\n",
        "                logits, _ = self.forward(input_idx)\n",
        "                probabilities = F.softmax(logits[:, -1, :] / temperature, dim=-1).cpu().numpy().reshape(-1)\n",
        "                chosen_index = self.top_k_top_p_filtering(probabilities, top_k=top_k, top_p=top_p)\n",
        "                next_char = decode([chosen_index])\n",
        "                tmp = torch.argmax(logits, dim=-1).cpu().tolist()[0]\n",
        "                # print(tmp)\n",
        "                print(f\"Output: {decode(tmp)}\")\n",
        "                print(\"==========\")\n",
        "                generated_text += next_char\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "\n",
        "    def top_k_top_p_filtering(self, probabilities, top_k=5, top_p=0.9):\n",
        "\n",
        "\n",
        "        def top_k_sampling(probabilities, k=5):\n",
        "            \"\"\"\n",
        "            Performs top-k sampling from a probability distribution.\n",
        "\n",
        "            :param probabilities: A 1D numpy array containing the probability distribution.\n",
        "            :param k: The number of top elements to consider for sampling.\n",
        "            :return: An index sampled from the top-k distribution.\n",
        "            \"\"\"\n",
        "            # Get indices of the top k probabilities\n",
        "            top_k_indices = np.argsort(probabilities)[-k:]\n",
        "\n",
        "            # Extract the top k probabilities\n",
        "            top_k_probabilities = probabilities[top_k_indices]\n",
        "\n",
        "            # Normalize the top k probabilities so they sum to 1\n",
        "            top_k_probabilities /= top_k_probabilities.sum()\n",
        "\n",
        "            # Sample from the top k elements\n",
        "            chosen_index = np.random.choice(top_k_indices, p=top_k_probabilities)\n",
        "\n",
        "            return chosen_index\n",
        "\n",
        "        def top_p_sampling(probabilities, p=0.9):\n",
        "            \"\"\"\n",
        "            Selects tokens from a probability distribution that have a cumulative probability\n",
        "            greater than the threshold p.\n",
        "\n",
        "            :param probabilities: A 1D numpy array containing the probability distribution.\n",
        "            :param p: The cumulative probability threshold (0 < p <= 1).\n",
        "            :return: An index sampled according to the top-p distribution.\n",
        "            \"\"\"\n",
        "            # print(f\"{probabilities=}\")\n",
        "            # Sort probabilities in descending order and also get the original indices\n",
        "            sorted_indices = np.argsort(probabilities)[::-1]\n",
        "            # print(f\"{sorted_indices=}\")\n",
        "            sorted_probabilities = probabilities[sorted_indices]\n",
        "\n",
        "            # print(f\"{sorted_probabilities=}\")\n",
        "\n",
        "            # Compute cumulative probabilities\n",
        "            cumulative_probabilities = np.cumsum(sorted_probabilities)\n",
        "            # print(f\"{cumulative_probabilities=}\")\n",
        "\n",
        "            # Find the threshold index\n",
        "            cutoff_index = np.where(cumulative_probabilities > p)[0][0]\n",
        "            # print(f\"{cutoff_index=}\")\n",
        "\n",
        "            # Filter out indices and probabilities that don't meet the threshold\n",
        "            filtered_indices = sorted_indices[:cutoff_index + 1]\n",
        "            # print(f\"{filtered_indices=}\")\n",
        "            filtered_probabilities = sorted_probabilities[:cutoff_index + 1]\n",
        "            # print(f\"{filtered_probabilities=}\")\n",
        "\n",
        "            # Normalize the filtered probabilities\n",
        "            filtered_probabilities /= filtered_probabilities.sum()\n",
        "            # print(f\"normalized_filtered_probabilities{filtered_probabilities}\")\n",
        "\n",
        "            # # Sample from the filtered distribution\n",
        "            # chosen_index = np.random.choice(filtered_indices, p=filtered_probabilities)\n",
        "            # print(chosen_index)\n",
        "            return filtered_probabilities\n",
        "\n",
        "        probabilities = top_p_sampling(probabilities, p=top_p)\n",
        "        chosen_index = top_k_sampling(probabilities, k=top_k)\n",
        "        return chosen_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njzrwwiv-mfB"
      },
      "source": [
        "### Training loop \n",
        "\n",
        "implement training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qWtn2uTwYUrY",
        "outputId": "46889c49-bed9-4ebd-f6bf-4ac1a9445fbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.3530, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 2008/20000 [00:47<07:15, 41.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7146, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 4007/20000 [01:32<05:16, 50.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6360, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 6010/20000 [02:17<04:57, 47.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6077, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 8007/20000 [03:04<05:13, 38.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6105, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 10007/20000 [03:46<03:17, 50.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 12007/20000 [04:28<03:14, 41.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6158, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 14010/20000 [05:13<01:54, 52.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5697, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 16006/20000 [05:53<01:22, 48.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5612, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 18009/20000 [06:35<00:38, 52.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5644, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [07:14<00:00, 46.03it/s]\n"
          ]
        }
      ],
      "source": [
        "model = GPT(vocab_size=len(chars), n_embd=64, n_head=4, n_layer=2, max_seq_len=32, dropout_rate=0.01).to('cuda') # make you are running this on the GPU\n",
        "max_iters = 20000\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.train()\n",
        "from tqdm import tqdm\n",
        "for iter in tqdm(range(max_iters)):\n",
        "    x, y = get_batch()\n",
        "    # print(x)\n",
        "\n",
        "    # break\n",
        "    _, loss = model(x, y)\n",
        "    if iter % 2000 == 0:\n",
        "        print(loss)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      },
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5l4soWviWG5M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: I tell you, fri\n",
            "Output:  hhll mou  sooe\n",
            "==========\n",
            "Input: I tell you, friq\n",
            "Output:  hhll mou  sooeu\n",
            "==========\n",
            "Input:  tell you, friqq\n",
            "Output: thll tou  sooeuu\n",
            "==========\n",
            "Input: tell you, friqqq\n",
            "Output: hrl tou  sooeuuu\n",
            "==========\n",
            "Input: ell you, friqqqq\n",
            "Output:  l tou  sooeuuuu\n",
            "==========\n",
            "Input: ll you, friqqqqq\n",
            "Output: l tou  sooeuuuuu\n",
            "==========\n",
            "Input: l you, friqqqqqq\n",
            "Output: ltour sooeuuuuuu\n",
            "==========\n",
            "Input:  you, friqqqqqqq\n",
            "Output: tour sooeuuuuuuu\n",
            "==========\n",
            "Input: you, friqqqqqqqq\n",
            "Output:  ur sooeuuuuuuuu\n",
            "==========\n",
            "Input: ou, friqqqqqqqqq\n",
            "Output: ur tooeuuuuuuuuu\n",
            "==========\n",
            "Input: u, friqqqqqqqqqq\n",
            "Output: r tooeuuuuuuuuuu\n",
            "==========\n",
            "Input: , friqqqqqqqqqqq\n",
            "Output:  tooeuuuuuuuuuuu\n",
            "==========\n",
            "Input:  friqqqqqqqqqqqq\n",
            "Output: tooeuuuuuuuuuuuu\n",
            "==========\n",
            "Input: friqqqqqqqqqqqqq\n",
            "Output:  oeuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: riqqqqqqqqqqqqqq\n",
            "Output:  nuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: iqqqqqqqqqqqqqqq\n",
            "Output: nuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n",
            "Input: qqqqqqqqqqqqqqqq\n",
            "Output: uuuuuuuuuuuuuuuu\n",
            "==========\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I tell you, friqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq'"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(start_char=\"I tell you, fri\", temperature=0.5, top_p=0.99, top_k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
